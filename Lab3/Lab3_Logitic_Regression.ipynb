{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Lab 3: Logistic Regression\n",
      "\n",
      "In this practice session, you are invited to train a logistic regression classifier using gradient descent method in a first time then a python optimization function. After that your classifier should predict if student will be admitted or not given his score in two main exams. \n",
      "\n",
      "In the second part, we will work with more complex data that represents the scores result of two test process on a manufactured microchip. Based on this data, you could use polynomial features to train a classifier that predict if a microchip with known tests result will be accepted or rejected. \n",
      "\n",
      "### Student scores data set\n",
      "#### Load data\n",
      "\n",
      "<font color=\"blue\">**Question 1: **</font>The *\"exams.txt\"* file contains 3 columns that represent the exam 1, exam 2 scores and the result of 100 students (0: Not admitted, 1: Admitted).\n",
      "- Open this file with a file editor to understand more the data. \n",
      "- Load the data in \"house_data\" variable and check its size.  \n",
      "**Hint:** You could use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "students_results = np.loadtxt('exams.txt',delimiter='\\t')  # ** your code here** \n",
      "\n",
      "# you could verify the size of the data using shape() function on numpy array house_data\n",
      "# ** your code here** \n",
      "#print(students_results.shape)\n",
      "#print(students_results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Extract and visualize features\n",
      "<font color=\"blue\">**Question 2: **</font> \n",
      "- Determine the number of student \"m\" from the input data \"students_results\".\n",
      "- Extract exam 1, exam 2 scores and the result columns respectively in \"x_1\", \"x_2\" and \"y\".  \n",
      "**Hint:** The shape of \"x_1\", \"x_2\" and \"y\" arrays should be (m,1) for the following questions and not (m,). You could use [newaxis numpy](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#basic-slicing-and-indexing) object to add a new axis of length one.\n",
      "- Determine the number of features \"n\" (number of columns of array \"X\")."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib notebook\n",
      "\n",
      "m = students_results.shape[0] # number of student\n",
      "\n",
      "x_1 = students_results[:,0,np.newaxis] # we add np.newaxis in the indexing to obtain an array \n",
      "x_2 = students_results[:,1,np.newaxis] # with shape (100,1) instead of (100,)\n",
      "X = np.concatenate((np.ones((m,1)),x_1,x_2),axis=1)\n",
      "\n",
      "n = X.shape[1]                # number of features\n",
      "y = students_results[:,2,np.newaxis] # we add np.newaxis in the indexing to obtain an array with shape (100,1) instead of (100,)\n",
      "\n",
      "# visualize data\n",
      "plt.figure(\"Visualize students results\",figsize=(9,5))\n",
      "plt.scatter(x_1[y==0], x_2[y==0],  color='red',label='fail')\n",
      "plt.scatter(x_1[y==1], x_2[y==1],  color='green',marker='+',s=80, label='success')\n",
      "plt.xlabel('Exam 1 score')\n",
      "plt.ylabel('Exam 2 score')\n",
      "plt.title('Adimitted/Not admitted Students')\n",
      "legend = plt.legend(loc='lower left', shadow=True, fontsize='x-large')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Sigmoid and MSE Cost Function\n",
      "As we see, the linear regression model is not suitable for classification problem (output=0/1) because it may result in $h_\\theta(x_i)=\\theta^\\top x_i<0$ or $h_\\theta(x_i)=\\theta^\\top x_i>1$. Moreover, the linear regression will be very sensitive to an additional data in case of classification problem. Hence, we choose little bit different model the logistic regression where : $$h_\\theta(x_i)=sigmoid(\\theta^\\top x_i)=\\frac{1}{1+e^{-\\theta^\\top x_i}}$$\n",
      "This new hypothesis formulas will ensure: $0\\leq h_\\theta(x_i)\\leq 1$  \n",
      "\n",
      "**Recall:** The Mean Squared Error (MSE) cost function equal: $$MSE(X) = \\frac{1}{2~m} \\sum_{i=1}^{m}{(h_\\theta(x_i) - y_i)^2}$$\n",
      "\n",
      "<font color=\"blue\">**Question 3: **</font>\n",
      "- Implement the \"sigmoid\" function that helps to calculate the hypothesis $h_\\theta$ (given in the previous equation). \n",
      "- Implement the \"MSE_cost_func\" function that evaluate and return the mean squared error according to the given equation (make a vectorized implementation)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from mpl_toolkits.mplot3d.axes3d import*\n",
      "from matplotlib import cm\n",
      "\n",
      "def sigmoid(z):\n",
      "    return np.ones(z.shape)/(1+np.exp(-z))\n",
      "\n",
      "def MSE_cost_func(theta):\n",
      "    J= 1/2*np.sum((sigmoid(np.dot(X,theta))-y)**2,axis=0)\n",
      "    return J/m  # this is the vectorized implementation\n",
      "\n",
      "# evaluate cost function at theta_init\n",
      "theta_init=np.array([[0],[0],[0]],dtype=float)\n",
      "cost = cost_func(theta_init)\n",
      "print(\"MSE cost function for theta={0} is : {1}\".format(theta_init[:,0],cost))\n",
      "\n",
      "theta_1 = np.linspace(-5,5,50) # you could also use: theta_1 = np.arange(100, 300, 2)\n",
      "theta_2 = np.linspace(-5,5,50)    # you could also use: theta_2 = np.arange(-1, 5, 0.05)\n",
      "theta_1, theta_2 = np.meshgrid(theta_1, theta_2)\n",
      "Theta=np.concatenate((np.zeros((*theta_1.shape,1)),theta_1[:,:,np.newaxis],theta_2[:,:,np.newaxis]),axis=-1)\n",
      "\n",
      "Thetabis=Theta.reshape((Theta.shape[0]*Theta.shape[1],Theta.shape[2]))\n",
      "Zbis =  MSE_cost_func(Thetabis.transpose())\n",
      "Z=Zbis.reshape((Theta.shape[0],Theta.shape[1]))\n",
      "#Z =  1/(2*m)*np.sum((sigmoid(np.dot(Theta,X.transpose()))-np.tile(y[np.newaxis,np.newaxis,:,0],(*theta_1.shape,1)))**2,axis=-1)\n",
      "\n",
      "print(\"Minimum value of MSE cost function detected on the plot: \",np.min(Z))\n",
      "print(\"Value of theta_1, theta_2 that minimize the MSE cost function: \",theta_1[np.argmin(np.min(Z,axis=1)),np.argmin(np.min(Z,axis=0))],theta_2[np.argmin(np.min(Z,axis=1)),np.argmin(np.min(Z,axis=0))])\n",
      "\n",
      "\n",
      "# the contour plot (projection of 3d plot on 2d plan)\n",
      "fig=plt.figure('Contour and Surface Plots for MSE cost function',figsize=(9,4))\n",
      "ax = fig.add_subplot(1, 2, 1)\n",
      "ctr = plt.contour(theta_1, theta_2, Z)\n",
      "plt.clabel(ctr, inline=1, fontsize=10)\n",
      "ax.set_title('Contour Plot')\n",
      "ax.set_xlabel('theta_1')\n",
      "ax.set_ylabel('theta_2')\n",
      "\n",
      "# the surface plot (3d plot)\n",
      "ax=fig.add_subplot(1, 2, 2,projection='3d')\n",
      "ax.plot_surface(theta_1,theta_2,Z,rstride=1,cstride=1,cmap=cm.coolwarm,linewidth=1,antialiased=True)\n",
      "ax.set_title('Surface Polt')\n",
      "ax.set_xlabel('theta_1')\n",
      "ax.set_ylabel('theta_2')\n",
      "ax.set_zlabel('Cost Funtion')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Logistic cost function\n",
      "<font color=\"blue\">**Question 4: **</font> \n",
      "- Implement the \"cost_func\" function that evaluate and return the logistic cost function given in the following equation (make a vectorized implementation): $$cost\\_func(x)=\\frac{-1}{m}\\sum_{i=1}^{m}\\left [y\\times log(h_\\theta(x))+(1-y)\\times log(1-h_\\theta(x))\\right ]$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cost_func(theta):\n",
      "    J=np.sum(-y*np.log(sigmoid(np.dot(X,theta))),axis=0)-np.sum((1-y)*np.log(1-sigmoid(np.dot(X,theta))),axis=0)\n",
      "    return J/m  \n",
      "\n",
      "# evaluate cost function at theta_init\n",
      "cost = cost_func(theta_init)\n",
      "print(\"Cost function for theta={0} is : {1}\".format(theta_init[:,0],cost))\n",
      "\n",
      "Z=np.zeros(theta_1.shape)\n",
      "for i in range(theta_1.shape[0]):\n",
      "    for j in range(theta_1.shape[1]):\n",
      "        for k in range(m):\n",
      "            if (y[k,0]==1):\n",
      "                if (sigmoid(np.dot(Theta[i,j,:],X[k,:]))==0):\n",
      "                    Z[i,j]+=100\n",
      "                else:\n",
      "                    Z[i,j]-=np.log(sigmoid(np.dot(Theta[i,j,:],X[k,:])))\n",
      "            else:\n",
      "                if (sigmoid(np.dot(Theta[i,j,:],X[k,:]))==1):\n",
      "                    Z[i,j]+=100\n",
      "                else:\n",
      "                    Z[i,j]-=np.log(1-sigmoid(np.dot(Theta[i,j,:],X[k,:])))\n",
      "Z=Z/m          \n",
      "#Y=np.tile(y[np.newaxis,np.newaxis,:,0],(*theta_0.shape,1))\n",
      "#Z =  -1/m*np.sum((Y*np.log(sigmoid(np.dot(Theta,X.transpose())))+(1-Y)*np.log(1-sigmoid(np.dot(Theta,X.transpose())))),axis=-1)\n",
      "\n",
      "print(\"Minimum value of cost function detected on the plot: \",np.min(Z))\n",
      "print(\"Value of theta_1, theta_2 that minimize the cost function: \",theta_0[np.argmin(np.min(Z,axis=1)),np.argmin(np.min(Z,axis=0))],theta_1[np.argmin(np.min(Z,axis=1)),np.argmin(np.min(Z,axis=0))])\n",
      "\n",
      "\n",
      "# the contour plot (projection of 3d plot on 2d plan)\n",
      "fig=plt.figure('Contour and Surface Plots Logistic cost function',figsize=(9,4))\n",
      "ax = fig.add_subplot(1, 2, 1)\n",
      "ctr = plt.contour(theta_1, theta_2, Z)\n",
      "plt.clabel(ctr, inline=1, fontsize=10)\n",
      "ax.set_title('Contour Plot')\n",
      "ax.set_xlabel('theta_1')\n",
      "ax.set_ylabel('theta_2')\n",
      "\n",
      "# the surface plot (3d plot)\n",
      "ax=fig.add_subplot(1, 2, 2,projection='3d')\n",
      "ax.plot_surface(theta_1,theta_2,Z,rstride=1,cstride=1,cmap=cm.coolwarm,linewidth=1,antialiased=True)\n",
      "ax.set_title('Surface Polt')\n",
      "ax.set_xlabel('theta_1')\n",
      "ax.set_ylabel('theta_2')\n",
      "ax.set_zlabel('Cost Funtion')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Gradient vector and gradient descent algorithm\n",
      "The gradient vector of the logistic cost function is calculated as following: $$\\nabla J(\\theta) = \\begin{bmatrix}\\frac{\\partial J(\\theta)}{\\partial \\theta_0}\n",
      "\\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_1}\n",
      "\\\\ \\vdots\n",
      "\\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n-1}}\n",
      "\\end{bmatrix}$$ \n",
      "where: $\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m}{(h_\\theta(x_i) - y)~x_j} ~~for~ j=0\\dots n-1$\n",
      "<font color=\"blue\">**Question 5: **</font> \n",
      "- Implement the \"grad_cost_func\" function that evaluates the gradient of logistic cost function at the point theta considering the $j^{th}$ component as given on the previous equation.\n",
      "- Implement the update equation of the gradient descent algorithm given by: \n",
      "$$\\theta=\\theta-\\alpha \\nabla J(\\theta)$$\n",
      "\n",
      "** Note:** The execution of this block of code could take several seconds."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "\n",
      "def grad_cost_func(theta):\n",
      "    g=(1/(m))*(np.dot(X.transpose(),(sigmoid(np.dot(X,theta))-y)))  # this is the vectorized implementation    \n",
      "    return g  \n",
      "\n",
      "# evaluate gradient of cost function at theta_init\n",
      "grad=grad_cost_func(theta_init)\n",
      "print(\"Gradient for theta={0} is : {1}\".format(theta_init[:,0],grad[:,0]))\n",
      "\n",
      "def grad_descent(grad_func, theta0,max_iter=1000,alpha=0.001,gtol=10**(-10)):\n",
      "    # max_iter: maximum number of iteration or steps to make\n",
      "    # alpha: steplength or learning rate\n",
      "    # gtol: gradient tolerance if gradient goes below this value we can say that our algorithm converge\n",
      "    i=0\n",
      "    J=[]\n",
      "    theta=theta0.copy()\n",
      "    while (i < max_iter):# and (np.sum(g**2) > gtol):\n",
      "        i+=1\n",
      "        g=grad_func(theta)\n",
      "        theta=theta-alpha*g\n",
      "        J.append(cost_func(theta))\n",
      "    return theta,J\n",
      "\n",
      "start_time = time.time()\n",
      "theta_opt,J1=grad_descent(grad_cost_func,theta_init,max_iter=100000,alpha=0.001)\n",
      "print(\"The gradient descent algorithm take {:.4f} s to finish calculation\".format(time.time()-start_time))\n",
      "print(\"The optimal value of theta that minimize cost function is: \",theta_opt[:,0])\n",
      "print(\"Final error = \",J1[len(J1)-1])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Selecting learning rate\n",
      "Execute the following code and visualize learning curves that describe the decrease of the cost function during the gradient descent iterations.\n",
      "\n",
      "<font color=\"blue\">**Question 6: **</font> \n",
      "- Set the learning rate \"alpha\" to the best value that helps to decrease quickly the cost function. \n",
      "\n",
      "** Note:** The execution of this block of code could take several seconds.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate the optimal value of theta that minimize cost function\n",
      "\n",
      "#theta_opt2,J2=grad_descent(grad_cost_func,theta_init,max_iter=100000,alpha=0.003)\n",
      "#theta_opt3,J3=grad_descent(grad_cost_func,theta_init,max_iter=100000,alpha=0.0001)\n",
      "\n",
      "plt.figure('Learning curves')\n",
      "plt.plot(range(len(J3)),J3,label='alpha=0.0001')\n",
      "plt.plot(range(len(J1)),J1,label='alpha=0.001')\n",
      "plt.plot(range(len(J2)),J2,label='alpha=0.003')\n",
      "\n",
      "plt.xlabel('number of iteration')\n",
      "plt.ylabel('cost function J(theta)')\n",
      "plt.title('cost function vs number of iteration')\n",
      "plt.legend(loc='best', shadow=True, fontsize='x-large')\n",
      "\n",
      "best_alpha = 0.003 # set the best value of alpha"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=\"blue\">**Question 7: **</font> \n",
      "- From the learning curves the cost function seems to decrease more. Hence, try to run the gradient descent with 1000000 (1 million) iterations and describe the difference between two cases.\n",
      "- Use the optimal theta calculated to predict the result of student who has a score of 11 in exam 1 and a score of 9.5 in exam 2.  \n",
      "\n",
      "** Note:** The execution of this block of code could take up to one minute."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from matplotlib.patches import Rectangle\n",
      "\n",
      "# calculate optimal theta\n",
      "start_time = time.time()\n",
      "theta_opt,J=grad_descent(grad_cost_func,theta_init,max_iter=1000000,alpha=best_alpha)\n",
      "print(\"The gradient descent algorithm take {:.4f} s to finish calculation\".format(time.time()-start_time))\n",
      "print(\"The optimal value of theta that minimize cost function is: \",theta_opt[:,0])\n",
      "print(\"Final error = \",J[len(J)-1])\n",
      "\n",
      "# predict student result\n",
      "exam_1 = 11\n",
      "exam_2 = 9.5\n",
      "result = sigmoid(np.dot(np.array([[1,exam_1,exam_2]]),theta_opt))\n",
      "print(\"The predicted result of student with {0} and {1} scores is: {2}\".format(exam_1,exam_2,['Not admitted','Admitted'][int(round(result[0,0]))]))\n",
      "print(\"With an admission probability: \",result)\n",
      "\n",
      "# plot learning curve\n",
      "plt.figure('Learning curve')\n",
      "plt.plot(range(len(J)),J,label='alpha=0.0003')\n",
      "plt.legend(loc='best', shadow=True, fontsize='x-large')\n",
      "\n",
      "# plot classifier decision boundries and data \n",
      "plt.figure('decision boundries')\n",
      "fail=plt.scatter(x_1[y==0], x_2[y==0],  color='red',label='fail')\n",
      "succ=plt.scatter(x_1[y==1], x_2[y==1],  color='green',marker='+',s=80, label='success')\n",
      "plt.plot([np.min(X[:,1])-0.5,np.max(X[:,1])+0.5],[(-theta_opt[0,0]-theta_opt[1,0]*(np.min(X[:,1])-0.5))/theta_opt[2,0],(-theta_opt[0,0]-theta_opt[1,0]*(np.max(X[:,1])+0.5))/theta_opt[2,0]],color=\"blue\")\n",
      "plt.xlabel('Exam 1 score')\n",
      "plt.ylabel('Exam 2 score')\n",
      "plt.title('Adimitted/Not admitted Students')\n",
      "extra = Rectangle((0, 0), 3, 4, fc=\"w\", fill=False, edgecolor=\"b\", linewidth=1)\n",
      "plt.legend([extra,fail,succ], (\"decision boundries\",\"fail\",\"success\"),loc='lower left')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#### Minimize cost function with python optimization function:\n",
      "We note that the gradient descent algorithm takes a lot of time and it is not suitable for optimizing complex  function like logistic cost function. Hence, we will use [fmin_bfgs](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_bfgs.html) optimization function from the [scipy](https://www.scipy.org/) library in python.\n",
      "<font color=\"blue\">**Question 8: **</font> \n",
      "- The \"fmin_bfgs\" function work with theta and gradient array in form (n,) and nor (n,1). re-implement cost function (\"cost_func2\") and gradient function (\"grad_cost_func2\") that deal with this kind of arrays.\n",
      "- Call \"fmin_bfgs\" function to calculate the optimal theta. This function take as parameters: the name of cost function (\"cost_func2\"), the name of gradient cost function (\"grad_cost_func2\") and the initial theta (\"theta0\")."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.optimize import fmin_bfgs\n",
      "\n",
      "def cost_func2(theta):\n",
      "    J=np.sum(-y*np.log(sigmoid(np.dot(X,theta[:,np.newaxis]))))-np.sum((1-y)*np.log(1-sigmoid(np.dot(X,theta[:,np.newaxis]))))\n",
      "    return J/m  \n",
      "\n",
      "def grad_cost_func2(theta):\n",
      "    g=(1/m)*(np.dot(X.transpose(),(sigmoid(np.dot(X,theta[:,np.newaxis]))-y)))  # this is the vectorized implementation\n",
      "    g.shape=(g.shape[0],)\n",
      "    return g  \n",
      "\n",
      "# calculate the optimal theta\n",
      "theta0=np.array([0,0,0],dtype=float)\n",
      "start_time = time.time()\n",
      "Thopt= fmin_bfgs(cost_func2,theta0,fprime=grad_cost_func2)\n",
      "print(\"The 'fmin_bfgs' function take {:.4f} s to finish calculation\".format(time.time()-start_time))\n",
      "print(\"The optimal value of theta that minimize cost function is: \",Thopt)\n",
      "print(\"Final error = \",cost_func2(Thopt))\n",
      "\n",
      "# plot linear model and data \n",
      "plt.figure('Linear decision boundries')\n",
      "fail=plt.scatter(x_1[y==0], x_2[y==0],  color='red',label='fail')\n",
      "succ=plt.scatter(x_1[y==1], x_2[y==1],  color='green',marker='+',s=80, label='success')\n",
      "plt.plot([np.min(X[:,1])-0.5,np.max(X[:,1])+0.5],[(-Thopt[0]-Thopt[1]*(np.min(X[:,1])-0.5))/Thopt[2],(-Thopt[0]-Thopt[1]*(np.max(X[:,1])+0.5))/Thopt[2]],color=\"blue\")\n",
      "plt.xlabel('Exam 1 score')\n",
      "plt.ylabel('Exam 2 score')\n",
      "plt.title('Adimitted/Not admitted Students')\n",
      "plt.legend([extra,fail,succ], (\"decision boundries\",\"fail\",\"success\"),loc='lower left')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Microchip testing data set\n",
      "The microchip data set contains 3 columns. The scores result of two test process on a manufactured microchips is presented in the 2 first columns. While the third column indicates if the corresponding microchip were accepted or rejected.\n",
      "\n",
      "<font color=\"blue\">**Question 9: **</font> \n",
      "- Load data from \"microchip.txt\" file and extract each columns.  \n",
      "**Hint:** You could use [loadtxt](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.loadtxt.html) function from numpy library.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load data\n",
      "microchip_data = np.loadtxt('microchip.txt',delimiter=',')  # ** your code here** \n",
      "\n",
      "# you could verify the size of the data using shape() function on numpy array house_data\n",
      "# ** your code here** \n",
      "#print(microchip_data.shape)\n",
      "#print(microchip_data)\n",
      "\n",
      "m = microchip_data.shape[0] # number of student\n",
      "\n",
      "x_1 = microchip_data[:,0,np.newaxis] # we add np.newaxis in the indexing to obtain an array \n",
      "x_2 = microchip_data[:,1,np.newaxis] # with shape (100,1) instead of (100,)\n",
      "\n",
      "y = microchip_data[:,2,np.newaxis] # we add np.newaxis in the indexing to obtain an array with shape (100,1) instead of (100,)\n",
      "\n",
      "# visualize data\n",
      "plt.figure(\"Visualize microchip data\",figsize=(9,5))\n",
      "plt.scatter(x_1[y==0], x_2[y==0],  color='red',label='fail')\n",
      "plt.scatter(x_1[y==1], x_2[y==1],  color='green',marker='+',s=80, label='success')\n",
      "plt.xlabel('Test 1 score')\n",
      "plt.ylabel('Test 2 score')\n",
      "plt.title('Accepted/Rejected Microchip')\n",
      "legend = plt.legend(loc='lower left', shadow=True, fontsize='x-large')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=\"blue\">**Question 10: **</font> \n",
      "- Implement the \"Poly_Features\" function that concatenate to data array the different possible power (below deg) of feature vector f1 and f2 as shown below:$$data=[data,~f_1,~ f_1^2,~ \\dots,~ f_1^{deg},~ f_2,~ f_2^2,~ \\dots,~ f_2^{deg}]$$"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "\n",
      "'''\n",
      "for i in range(1,degree+1):\n",
      "    for j in range(i+1):\n",
      "        if ((i*j)!=2):\n",
      "            X = np.concatenate((X,x_1**(i-j)*x_2**j),axis=1)\n",
      "'''\n",
      "def Poly_Features(data,f1,f2,deg):\n",
      "    for i in range(1,deg+1):\n",
      "        data = np.concatenate((data,f1**i),axis=1)\n",
      "    for j in range(1,deg+1):\n",
      "        data = np.concatenate((data,f2**j),axis=1)\n",
      "    return data\n",
      "\n",
      "degree=4           # degree of polynomial feature\n",
      "X=np.ones((m,1))   # initialize X array\n",
      "\n",
      "# add polynomial features to the array data X\n",
      "X = Poly_Features(X,x_1,x_2,degree)\n",
      "n = X.shape[1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<font color=\"blue\">**Question 11: **</font> \n",
      "- Call \"fmin_bfgs\" function to calculate the optimal theta. This function take as parameters: the name of cost function (\"cost_func2\"), the name of gradient cost function (\"grad_cost_func2\") and the initial theta (\"theta0\").\n",
      "- Use the optimal theta calculated to predict a microchip result that has a score of 0.5 in test 1 and a score of -0.5 in test 2.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# calculate optimal theta\n",
      "theta0=np.zeros((n,))\n",
      "Thopt = fmin_bfgs(cost_func2,theta0,fprime=grad_cost_func2) \n",
      "print(\"The optimal value of theta that minimize cost function is: \",Thopt)\n",
      "print(\"Final error = \",cost_func2(Thopt))\n",
      "\n",
      "# predit microchip result\n",
      "test_1 = 0.5\n",
      "test_2 = -0.5\n",
      "Test = Poly_Features(np.ones((1,1)),np.array([[test_1]]),np.array([[test_2]]),degree)\n",
      "result = sigmoid(np.dot(Test,Thopt))\n",
      "print(\"The predicted result of microchip with {0} and {1} scores is: {2}\".format(test_1,test_2,['Rejected','Accepted'][int(round(result[0]))]))\n",
      "print(\"With an acceptance probability: \",result)\n",
      "\n",
      "# calculate the mesh grid for contour plot\n",
      "u1=np.linspace(-1,1.5,50)\n",
      "u2=np.linspace(-1,1.5,50)\n",
      "u1, u2 = np.meshgrid(u1, u2)\n",
      "\n",
      "X3=np.ones((*u1.shape,1))\n",
      "for i in range(1,degree+1):\n",
      "    X3 = np.concatenate((X3,u1[...,np.newaxis]**i),axis=-1)\n",
      "for j in range(1,degree+1):\n",
      "    X3 = np.concatenate((X3,u2[...,np.newaxis]**j),axis=-1)\n",
      "    \n",
      "Z=np.dot(X3,Thopt)\n",
      "\n",
      "# plot descision boundries\n",
      "plt.figure(\"Microchip decision boundries\",figsize=(7,5))\n",
      "fail=plt.scatter(x_1[y==0], x_2[y==0],  color='red',label='fail')\n",
      "success=plt.scatter(x_1[y==1], x_2[y==1],  color='green',label='success')\n",
      "plt.xlabel('house area (m\u00b2)')\n",
      "plt.ylabel('house price (1000\u20ac)')\n",
      "plt.title('house area vs price')\n",
      "ctr = plt.contour(u1, u2, Z,0,colors=\"blue\",label='decision boundries')\n",
      "plt.legend([extra,fail,succ], (\"decision boundries\",\"fail\",\"success\"),loc='best')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": null
    }
   ],
   "metadata": {}
  }
 ]
}